{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZH2K5LjNxDb"
      },
      "source": [
        "# HW2 - Q Actor Critics - DQN, DDPG, SAC\n",
        "\n",
        "This assignment builds to a simple Soft Actor Critic (2018) by progressing from predecessor algorithms: <br> Deep Q Networks (2013) and Deep Deterministic Policy Gradients (2015). They all build on tabular Q learning (\\~1989). Note, many variations of these algorithms exist. Please use the math contained in this notebook for the coding sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqv2aln3DDCz"
      },
      "source": [
        "# 0. Warm Up Questions [30 pts total; 2 pt each]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS-8empJz8Iu"
      },
      "source": [
        "\n",
        "Answer each question concisely. One sentence, one formula, one line of code, etc. Use of $\\LaTeX$ formatting for math is encouraged.\n",
        "\n",
        "1.    How does the Q function $Q(s_t, a_t)$ relate to sum of discounted rewards $\\sum_{t=0}^T \\gamma^t r_t$?<br>\n",
        "Type answer here ..\n",
        "\n",
        "2.    How does the Q function $Q(s_t, a_t)$ relate to $Q(s_{t+1}, a_{t+1})$?<br>\n",
        "\n",
        "3.    When Q is accurate are these definitions equivalent?<br>\n",
        "\n",
        "4.    Whats the loss for a neural approximation to the Q network?<br>\n",
        "\n",
        "5.    In the discrete case, how do you select actions given an accurate Q network?<br>\n",
        "\n",
        "6.    In DQN, for an environment with 5 continuous states and 3 discrete action choices: Whats the input and output size of the Q network?<br>\n",
        "\n",
        "7.    In DDPG, for an environment with 5 continuous states and 3 continuous action: Whats the input and output size of the Q network?<br>\n",
        "\n",
        "8.    In DQN, what is a target network and why do we need it?<br>\n",
        "\n",
        "9.    In DQN, what is a replay buffer and why do we need it?<br>\n",
        "\n",
        "10.    Explain this inequality $\\mathbb{E}[\\max(C_1, C_2)] \\geq \\max(\\mathbb{E}[C_1], \\mathbb{E}[C_2])$, assuming $C_1$ and $C_2$ are random variables representing the probability of getting heads when flipping two fair coins. (This is the basis for double Q learning in Double DQN and SAC.)<br>\n",
        "\n",
        "11.    Do off policy algorithms use a replay buffer?<br>\n",
        "\n",
        "12.    What does 'with torch.no_grad():' do and why should you use it when calling target networks but not regular networks?<br>\n",
        "\n",
        "13.    Why do you need a policy network in DDPG but not DQN, and what is the DDPG policy loss.<br>\n",
        "\n",
        "14.    Compare and contrast hard and soft target network updates.<br>\n",
        "\n",
        "15. In [InvertedPendulum-v5](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) what are the physical meanings of states and actions and are they discrete or continuous?<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdzpOoeBR9Ql"
      },
      "source": [
        "# Boiler Plate\n",
        "####(read through atleast once)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWpQBY3YKGAX"
      },
      "source": [
        "## Imports and Set up\n",
        "Installs gymnasium, imports deep learning libs, sets torch device. **You shouldnt need to change this code.**\n",
        "\n",
        "This notebook should work with CPU or GPU. To change: **click Runtime (top left of notebook) -> Change runtime type -> select a CPU/GPU -> Save**. I'd recommend debugging on the CPU (to save available GPU time) and doing full runs on GPU (to increase training speed). Regardless, these environments should solve within minutes even on the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aN-Jray4N018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: gymnasium[mujoco] in /home/jblevins32/anaconda3/lib/python3.11/site-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (3.2.7)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.1.0)\n",
            "Requirement already satisfied: etils[epath] in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.0)\n",
            "Requirement already satisfied: glfw in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.8.0)\n",
            "Requirement already satisfied: pyopengl in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2024.12.0)\n",
            "Requirement already satisfied: importlib_resources in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /home/jblevins32/anaconda3/lib/python3.11/site-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install gymnasium[mujoco]\n",
        "!apt install -y libgl1-mesa-glx libosmesa6 libglfw3 patchelf\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "from torch import nn, zeros\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# random seeds for reproducability\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZXqBcnlJbar"
      },
      "source": [
        "## Replay Buffer\n",
        "This is boiler plate code that lets your off-policy algorithms store their interactions with the environment. **You shouldn't need to change this code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tY6FSSzcN1MX"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        self.buffer = deque(maxlen=1_000_000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        transitions = list(zip(state, action, reward, next_state, 1 - torch.Tensor(done)))\n",
        "        self.buffer.extend(transitions)\n",
        "\n",
        "    def sample(self):\n",
        "        batch = random.sample(self.buffer, self.batch_size)\n",
        "        return [torch.stack(e).to(device) for e in zip(*batch)]  # states, actions, rewards, next_states, not_dones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEHCeEYVJsB8"
      },
      "source": [
        "## DRL Rollout\n",
        "Boiler plate code that initiates parallel environments and stores your agents $(s,a,r,s')$ interactions in a replay buffer. Also logs some stats to tensorboard. **You shouldn't need to change this code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Kc2yVA-CN1W-"
      },
      "outputs": [],
      "source": [
        "class DRL:\n",
        "    def __init__(self):\n",
        "        self.n_envs = 32\n",
        "        self.n_steps = 128\n",
        "\n",
        "        self.envs = gym.vector.SyncVectorEnv(\n",
        "            [lambda: gym.make(\"InvertedPendulum-v5\", reset_noise_scale=0.2) for _ in range(self.n_envs)])\n",
        "\n",
        "        self.replay_buffer = ReplayBuffer()\n",
        "\n",
        "    def rollout(self, agent, i):\n",
        "        \"\"\"Collect experience and store it in the replay buffer\"\"\"\n",
        "\n",
        "        obs = torch.Tensor(self.envs.reset()[0])\n",
        "\n",
        "        total_rewards = torch.zeros(self.n_envs)\n",
        "\n",
        "        for _ in range(self.n_steps):\n",
        "            with torch.no_grad():\n",
        "                actions = agent.get_action(obs.to(device), noisy=True).cpu()\n",
        "            next_obs, rewards, done, truncated, _ = self.envs.step(actions.numpy())\n",
        "            next_obs, rewards = torch.Tensor(next_obs), torch.Tensor(rewards)\n",
        "            # reward scaling by .01 keeps sum of rewards near 1, increases stability\n",
        "            self.replay_buffer.store(obs, actions, rewards*.01, next_obs, done | truncated)\n",
        "            obs = next_obs\n",
        "\n",
        "            total_rewards += rewards\n",
        "\n",
        "        writer.add_scalar(\"stats/Rewards\", total_rewards.mean().item() / self.n_steps, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "e5DIP5l2eb3z"
      },
      "outputs": [],
      "source": [
        "# @title Visualization code. Used later.\n",
        "import os\n",
        "from gym.wrappers import RecordVideo\n",
        "from IPython.display import Video, display, clear_output\n",
        "\n",
        "# Force MuJoCo to use EGL for rendering (important for Colab)\n",
        "os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
        "\n",
        "def visualize(agent):\n",
        "    \"\"\"Visualize agent with a custom camera angle.\"\"\"\n",
        "\n",
        "    # Create environment in rgb_array mode\n",
        "    env = gym.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\", reset_noise_scale=0.2)\n",
        "\n",
        "    # Apply video recording wrapper\n",
        "    env = RecordVideo(env, video_folder=\"./\", episode_trigger=lambda x: True)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    # Access the viewer object through mujoco_py\n",
        "    viewer = env.unwrapped.mujoco_renderer.viewer  # Access viewer\n",
        "    viewer.cam.distance = 3.0     # Set camera distance\n",
        "    viewer.cam.azimuth = 90       # Rotate camera around pendulum\n",
        "    viewer.cam.elevation = 0   # Tilt the camera up/down\n",
        "\n",
        "\n",
        "    for t in range(512):\n",
        "        with torch.no_grad():\n",
        "            actions = agent.get_action(torch.Tensor(obs).to(device)[None, :])[:, 0]\n",
        "        obs, _, done, _= env.step(actions.cpu().numpy())\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # Display the latest video\n",
        "    clear_output(wait=True)\n",
        "    display(Video(\"./rl-video-episode-0.mp4\", embed=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWsqAz6VRypQ"
      },
      "source": [
        "# Tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx4YzpR71tjp"
      },
      "source": [
        "This will launch an interactive tensorboard window within collab. It will display rewards in (close to) real time while your agents are training. You'll likely have to refresh if its not updating (circular arrow to right in the orange bar). **You shouldn't need to change this code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jCFMyCuIPHN3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 9628), started 4:20:12 ago. (Use '!kill 9628' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qItDHaIL_z1Y"
      },
      "source": [
        "_________________________________\n",
        "# 1. Deep Q Networks (DQN) [30 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8_pui26OT0E"
      },
      "source": [
        "\n",
        "1.   Define your Q network and Q target network [5 pts]\n",
        "2.   Define the Q network optimizer [5 pts]\n",
        "3.   Define the Q loss [15 pts]\n",
        "3.   Conceptual questions [5 pts]\n",
        "_________________________________\n",
        "#### Background\n",
        "\n",
        "DQN is an off-policy reinforcement learning algorithm that extends Q-learning using deep neural networks. It is designed for environments with discrete action spaces and was used to achieve human-level performance in Atari games in a seminal 2013 [paper](https://arxiv.org/abs/1312.5602). Its key innovations relative to naive neural fitted Q iteration include replay buffers (which decorrelate samples) and target networks (which give Q learning a stationary target to converge to).\n",
        "\n",
        "We will use DQN to solve a continuous action space problem by discretizing. We map discete indices $[0, 1]$ to continuous actions $[-3, 3]$ and vis versa.\n",
        "___________________________________\n",
        "\n",
        "#### Temporal Difference Q Loss for DQN:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=0}^N \\{Q_\\theta(s_t, a_t) - q_{\\text{target}}\\}^2\\\\\n",
        "q_{\\text{target}} = r_t + \\gamma \\max_{a_{t+1}}Q_{\\theta_\\text{target}}(s_{t+1}, a_{t+1}) \\cdot \\text{not\\_done}_t\n",
        "$$\n",
        "\n",
        "or equivalenty, more concisely:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{Q_\\theta(s, a) - (r_t + \\gamma \\max_{a'}Q_{\\theta_\\text{target}}(s', a') \\cdot \\text{not\\_done})\\}^2]\n",
        "$$\n",
        "(hint: Don't actually use a for loop. Use torch's batched operations for greater training speed.)\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L} $ is the Q net loss; a function of Q network parameters $\\theta$\n",
        "- $N$ is the size of the minibatch\n",
        "-$Q_\\theta$ is the Q network parametrized by $\\theta$\n",
        "-$s_t$ is state at timestep $t$\n",
        "-$a_t$ is action at timestep $t$\n",
        "-$r_t$ is reward at timestep $t$\n",
        "-$\\gamma$ is the discount factor on rewards\n",
        "-$Q_{\\theta_\\text{target}}$ is the Q target network parametrized by ${\\theta_\\text{target}}$\n",
        "-$s_{t+1}$ or $s'$ is state at timestep $t+1$ (hint: comes from replay buffer)\n",
        "-$a_{t+1}$ or $a'$ is action at timestep $t+1$ (hint: implied from max next Q)\n",
        "-$\\text{not\\_done}_t$ or $\\text{not\\_done}$ is the not done flag for timestep $t$ indicating state $s_t$ is not terminal (i.e. Q next should be considered)\n",
        "-$\\mathbb{E}$ is the expectation or average over the minibatch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NRJPEtLON1gw"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        self.n_actions = n_actions\n",
        "        self.exploration_rate = 1.\n",
        "        torch.manual_seed(0)  # for fair comparison\n",
        "\n",
        "        # todo: student code here\n",
        "        # Define Q-network, hint: dont forget .to(device), use atleast 1 nonlinear activation. Outputs values for each action\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(n_obs, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)\n",
        "        ).to(device)\n",
        "\n",
        "        # Define Q-target network, hint: deepcopy\n",
        "        self.q_target_net = copy.deepcopy(self.q_net).to(device)\n",
        "\n",
        "        # Define optimizer, hint: Adam, learning rate 3e-4 is a good place to start but feel free to try others\n",
        "        self.optimizer = Adam(params=self.q_net.parameters(), lr=3e-4)\n",
        "        # end student code\n",
        "\n",
        "\n",
        "    def get_action(self, states, noisy=False):\n",
        "        if noisy and torch.rand(1).item() < self.exploration_rate:\n",
        "            # Random action with probability self.exploration_rate\n",
        "            actions = torch.randint(0, self.n_actions, (states.shape[0],1))\n",
        "        else:\n",
        "            # Greedy action selection\n",
        "            with torch.no_grad(): actions = self.q_net(states).argmax(dim=-1, keepdim=True)\n",
        "        return (actions.float() - 0.5) * 6  # maps discrete [0, 1] to continuous [-3., 3.]\n",
        "\n",
        "\n",
        "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, gamma=.99):\n",
        "        actions = (actions/6 + .5).long() # maps continous [-3., 3.] to discrete [0, 1]\n",
        "\n",
        "        # todo: student code here\n",
        "        with torch.no_grad():\n",
        "            # hint: compute Q for all next states using q target network\n",
        "            # States: batch_size x n_obs\n",
        "            all_next_Q = self.q_target_net(next_states)\n",
        "\n",
        "        # hint: take the max next q over actions\n",
        "        max_next_Q, _ = torch.max(all_next_Q, dim=-1)\n",
        "\n",
        "        # hint: calculate q_target equation\n",
        "        q_target = rewards + gamma*max_next_Q*not_dones\n",
        "\n",
        "        # hint: compute the q values of all actions in state using q network\n",
        "        all_Q = self.q_net(states)\n",
        "\n",
        "        # hint: gather the q values of the actions that were actually taken\n",
        "        Q = all_Q.gather(1, actions).squeeze(-1)\n",
        "\n",
        "        # hint: compute Mean Squared Error loss between Q and q_target\n",
        "        criterion = nn.MSELoss()\n",
        "        loss = criterion(Q,q_target)\n",
        "        # end student code\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def update(self, replay_buffer, i):\n",
        "        for _ in range(64):\n",
        "            loss = self.get_q_loss(*replay_buffer.sample())\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        writer.add_scalar(\"loss/q loss\", loss.item(), i)\n",
        "\n",
        "        # Periodic hard update Q-target network to Q-network\n",
        "        if i % 16 == 0:\n",
        "            self.q_target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "        # decay and log exploration rate\n",
        "        self.exploration_rate = max(self.exploration_rate * 0.985, 0.05)\n",
        "        writer.add_scalar(\"stats/exploration rate\", self.exploration_rate, i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "FMlfTGEMVmH9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: DQN Q nets appears correct!\n",
            "Test passed: DQN Optimizer appears correct!\n",
            "Test passed: DQN loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title DQN Unit Tests (must run DQN Agent cell above first)\n",
        "def DQN_q_net():\n",
        "    a = DQN(16, 7)\n",
        "    assert a.q_net is not None, \"q_net not initialized\"\n",
        "    assert a.q_target_net is not None, \"q_target_net is not initialized\"\n",
        "    r = torch.randn(8, 16).to(device)\n",
        "    assert a.q_net(r).shape == (8, 7) and \\\n",
        "    isinstance(list(a.q_net.children())[-1], nn.Linear), \\\n",
        "    f\"Network not initialized correctly\"\n",
        "    assert a.q_target_net(r).shape == (8, 7) and \\\n",
        "    isinstance(list(a.q_target_net.children())[-1], nn.Linear), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "    print(\"Test passed: DQN Q nets appears correct!\")\n",
        "DQN_q_net()\n",
        "\n",
        "def test_DQN_optimizer():\n",
        "    a = DQN(16, 7)\n",
        "    assert a.optimizer is not None, \"Optimizer is not initialized\"\n",
        "    assert set(p for p in a.q_net.parameters())  == \\\n",
        "    set(p for group in a.optimizer.param_groups for p in group['params']),\\\n",
        "    \"Optimizer does not match q_net parameters\"\n",
        "    print(\"Test passed: DQN Optimizer appears correct!\")\n",
        "test_DQN_optimizer()\n",
        "\n",
        "def DQN_loss():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = (torch.randint(0, 2, (batch_size, n_actions)).float() - 0.5) * 6\n",
        "    r = torch.rand((batch_size,))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "    not_dones = torch.randint(0, 2, (batch_size,)).float()\n",
        "\n",
        "    dqn = DQN(4, 2)\n",
        "    torch.manual_seed(0)\n",
        "    dqn.q_net = nn.Linear(4, 2) # you should not use this architecture..\n",
        "    dqn.q_target_net = nn.Linear(4, 2)\n",
        "    loss = dqn.get_q_loss(s, a, r, s_, not_dones)\n",
        "    assert abs(loss.item() - (0.1567)) < 1e-4, \\\n",
        "    \"DQN loss does not match expected value.\"\n",
        "    print(\"Test passed: DQN loss appears correct!\")\n",
        "\n",
        "DQN_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-AfCDYn7O-BN"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m     10\u001b[0m     drl\u001b[38;5;241m.\u001b[39mrollout(dqn, i)\n\u001b[0;32m---> 11\u001b[0m     dqn\u001b[38;5;241m.\u001b[39mupdate(drl\u001b[38;5;241m.\u001b[39mreplay_buffer, i)\n",
            "Cell \u001b[0;32mIn[12], line 66\u001b[0m, in \u001b[0;36mDQN.update\u001b[0;34m(self, replay_buffer, i)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, replay_buffer, i):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m64\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_q_loss(\u001b[38;5;241m*\u001b[39mreplay_buffer\u001b[38;5;241m.\u001b[39msample())\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     68\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mDQN.get_q_loss\u001b[0;34m(self, states, actions, rewards, next_states, not_dones, gamma)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# todo: student code here\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# hint: compute Q for all next states using q target network\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# States: batch_size x n_obs\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     all_next_Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_target_net(next_states)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# hint: take the max next q over actions\u001b[39;00m\n\u001b[1;32m     45\u001b[0m max_next_Q, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(all_next_Q, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# tensorboard label can be changed with e.g. f'runs/unique_hyperparam_test'\n",
        "writer = SummaryWriter(log_dir=f'runs/DQN')\n",
        "\n",
        "drl = DRL()\n",
        "dqn = DQN(n_obs=4, n_actions=2)\n",
        "\n",
        "# takes ~5-10 minutes on google colab gpus\n",
        "for i in range(512):\n",
        "\n",
        "    drl.rollout(dqn, i)\n",
        "    dqn.update(drl.replay_buffer, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufRkeb6sPwuS"
      },
      "outputs": [],
      "source": [
        "visualize(dqn) # run again to see a different rollout\n",
        "print(\"DQN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG44n8Ks2QC-"
      },
      "source": [
        "**DQN Conceptual Question 1 - Target Networks:**\n",
        "\n",
        "DQN uses target networks to stabilize training. DQN Loss:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{Q_\\theta(s, a) - (r_t + \\gamma \\max_{a'}Q_{\\theta_\\text{target}}(s', a') \\cdot \\text{not\\_done})\\}^2]\n",
        "$$\n",
        "But, what if you didn't use a target network:\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{Q_\\theta(s, a) - (r_t + \\gamma \\max_{a'}Q_{\\theta}(s', a') \\cdot \\text{not\\_done})\\}^2]\n",
        "$$\n",
        "\n",
        "(Optionally, make this actual change in code and observe training results for yourself. Its a one line change. Make sure to revert or comment it before submitting. Any code change is not for credit, but may aid understanding. You can change the name of the tensorboard run for direct visual comparison.)\n",
        "\n",
        "In 1 or 2 sentences, what could happen to your Q loss over the course of training if you modified the DQN loss equation so that a target network is not used? Why?\n",
        "\n",
        "\n",
        "**Type answer here...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDTv7DL8QTq"
      },
      "source": [
        "**DQN Conceptual Question 2 - Double DQN:**\n",
        "\n",
        "The loss function for Double DQN improves upon standard DQN by using the Q-network to select the best action and the target Q-network to evaluate it:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_\\text{double dqn}(\\theta) = \\mathbb{E} \\{ ( Q_{\\theta}(s, a) - [ r_t + \\gamma Q_{\\theta_{\\text{target}}}(s', \\arg\\max_{a'} Q_{\\theta}(s', a')) \\cdot \\text{not\\_done} ] )^2 \\}\n",
        "$$\n",
        "\n",
        "(Optionally, implement this in code and observe the training, but comment or revert changes before submitting.)\n",
        "\n",
        "In 1 or 2 sentences, explain the intuition behind why this might improve performance.\n",
        "\n",
        "**Type answer here**..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ1i0MUp_3rT"
      },
      "source": [
        "_________________________________\n",
        "# 2. Deep Deterministic Policy Gradients (DDPG) [40 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68qST7yEf39l"
      },
      "source": [
        "\n",
        "1.   Define your Q network, Q target network, policy network, and policy target network [5 pts]\n",
        "2.   Define the Q network optimizer and policy network optimizer [5 pts]\n",
        "3.   Define the Q loss [15 pts] and policy loss [10 pts]\n",
        "3.   Conceptual questions [5 pts]\n",
        "_________________________________\n",
        "#### Background\n",
        "\n",
        "DDPG is an off-policy reinforcement learning algorithm that extends DQN to continuous action spaces. It is based off a theortical publication called Deterministic Policy Gradients. It solved many robotics tasks in a seminal 2015 [publication](https://arxiv.org/abs/1509.02971). Its key innovations relative to DQN are (1) a policy network which is trained to produce deterministic, continous actions that maximize the Q function, and (2) soft target updates.\n",
        "\n",
        "We will use it to solve a continuous action space environment natively, without discretization.\n",
        "________________________________\n",
        "\n",
        "#### DQN vs DDPG\n",
        "\n",
        "**Q networks**: Q networks in DQN take in states and output the Q value for each action. Q networks in the continuous case take in both the state and action and output a single Q estimate.\n",
        "\n",
        "**Policies**: The policy in DQN comes from taking the action corresponding to the max Q value over discrete options. The policy in DDPG comes from training a network which takes in states/observations and outputs continuous actions that are trained to maximize Q. Since the policy approximates the max operator, explicit $\\max_{a'}$ is dropped from the Temporal Difference Q loss.\n",
        "________________________________\n",
        "\n",
        "#### Temporal Difference Q Loss for DDPG:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{Q_\\theta(s, a) - (r_t + \\gamma Q_{\\theta_\\text{target}}(s', a') \\cdot \\text{not\\_done})\\}^2]\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L} $ is the Q net loss; a function of Q network parameters $\\theta$\n",
        "- $E$ is the expectation or average over the minibatch\n",
        "-$Q_\\theta$ is the Q network parametrized by $\\theta$\n",
        "-$s_t$ is state at timestep $t$\n",
        "-$a_t$ is action at timestep $t$\n",
        "-$r_t$ is reward at timestep $t$\n",
        "-$\\gamma$ is the discount factor on rewards\n",
        "-$Q_{\\theta_\\text{target}}$ is the Q target network parametrized by ${\\theta_\\text{target}}$\n",
        "-$s'$ is state at timestep $t+1$ (hint: comes from replay buffer)\n",
        "-$a'$ is action at timestep $t+1$ (hint: comes from policy target network applied to next state. get_target_action())\n",
        "- $E$ is the expectation or average over the minibatch\n",
        "\n",
        "_______________________________\n",
        "\n",
        "#### Policy Loss for DDPG:\n",
        "$$\n",
        "\\mathcal{L}(\\theta_p) = -\\mathbb{E}[Q_\\theta(s, a)]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L} $ is the policy loss; a function of policy parameters $\\theta_p$\n",
        "- $E$ is the expectation or average over the minibatch\n",
        "-$Q_\\theta$ is the Q network parametrized by $\\theta$\n",
        "- $s$ is state\n",
        "- $a$ is the deterministic action the policy would take in state $s$ (hint: get_action())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NinRktLJf21H"
      },
      "outputs": [],
      "source": [
        "class DDPG:\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        self.exploration_rate = 1.\n",
        "        torch.manual_seed(0)\n",
        "\n",
        "        # todo: student code here\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(n_obs+n_actions,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        ).to(device)\n",
        "\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(n_obs,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,n_actions)\n",
        "        ).to(device)\n",
        "\n",
        "        self.q_target_net = copy.deepcopy(self.q_net).to(device)\n",
        "        self.policy_target_net = copy.deepcopy(self.policy).to(device)\n",
        "\n",
        "        self.q_optimizer = Adam(params=self.q_net.parameters(),lr=3e-3)\n",
        "        self.policy_optimizer = Adam(params=self.policy.parameters(),lr=3e-3)\n",
        "        # end student code\n",
        "\n",
        "\n",
        "    def get_action(self, states, noisy=False):\n",
        "        actions = self.policy(states)\n",
        "        if noisy:\n",
        "          actions += torch.normal(0, self.exploration_rate, size=actions.shape).to(device)\n",
        "        return actions.clamp(-3, 3)\n",
        "\n",
        "    def get_target_action(self, next_states):\n",
        "        actions = self.policy_target_net(next_states)\n",
        "        return actions.clamp(-3, 3)\n",
        "\n",
        "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, gamma=.99):\n",
        "\n",
        "        #todo: student code here\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 1) Get next actions from target policy\n",
        "            next_actions = self.policy_target_net(next_states)\n",
        "\n",
        "            # 2) Get target value from next states and next actions\n",
        "            next_state_action_vec = torch.cat((next_states,next_actions),dim=-1)\n",
        "            q_next = self.q_target_net(next_state_action_vec)\n",
        "\n",
        "        # 3) Get value from current states and current actions\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "        q = self.q_net(state_action_vec)\n",
        "\n",
        "        # 4) Get target q \n",
        "        q_target = rewards.unsqueeze(-1) + gamma*q_next*not_dones.unsqueeze(-1)\n",
        "\n",
        "        # 5) Get q loss\n",
        "        criterion = nn.MSELoss()\n",
        "        Q_loss = criterion(q,q_target)\n",
        "        # end student code\n",
        "\n",
        "        return Q_loss\n",
        "\n",
        "\n",
        "    def get_policy_loss(self, states):\n",
        "        # todo: student code here\n",
        "        # 1) Get actions\n",
        "        actions = self.policy(states)\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "\n",
        "        policy_loss = -torch.mean(self.q_net(state_action_vec))\n",
        "\n",
        "        # end student code\n",
        "        return policy_loss\n",
        "\n",
        "\n",
        "    def update(self, replay_buffer, i):\n",
        "\n",
        "        for _ in range(64):\n",
        "            loss = self.get_q_loss(*replay_buffer.sample())\n",
        "            self.q_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.q_optimizer.step()\n",
        "        writer.add_scalar(\"loss/q loss\", loss.item(), i)\n",
        "\n",
        "        for _ in range(4):\n",
        "            states, _, _, _, _ = replay_buffer.sample()\n",
        "            loss = self.get_policy_loss(states)\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.policy_optimizer.step()\n",
        "        writer.add_scalar(\"loss/ - policy loss\", -loss.item(), i)\n",
        "\n",
        "        tau = 0.1  # Continual soft target update\n",
        "        for target_param, param in zip(self.q_target_net.parameters(), self.q_net.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(self.policy_target_net.parameters(), self.policy.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        self.exploration_rate  = max(self.exploration_rate  * 0.985, 0.05)\n",
        "        writer.add_scalar(\"stats/exploration rate\", self.exploration_rate , i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: DDPG nets appear correct!\n",
            "Test passed: DDPG optimizer appears correct!\n",
            "Test passed: DDPG q loss appears correct!\n",
            "Test passed: DDPG policy loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title DDPG Unit Tests (must run DDPG Agent cell above first)\n",
        "def DDPG_nets():\n",
        "    a = DDPG(16, 7)\n",
        "    assert a.q_net is not None, \"q_net not initialized\"\n",
        "    assert a.q_target_net is not None, \"q_target_net is not initialized\"\n",
        "    assert a.policy is not None, \"policy is not initialized\"\n",
        "    assert a.policy_target_net is not None, \"policy_target_net is not initialized\"\n",
        "    r = torch.randn(8, 23).to(device)\n",
        "\n",
        "    # this doesnt check if target is the same architecture as q\n",
        "    # but it should be\n",
        "    assert a.q_net(r).shape == (8, 1) and \\\n",
        "    isinstance(list(a.q_net.children())[-1], nn.Linear), \\\n",
        "    f\"Network not initialized correctly\"\n",
        "    assert a.q_target_net(r).shape == (8, 1) and \\\n",
        "    isinstance(list(a.q_target_net.children())[-1], nn.Linear), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "\n",
        "    r = torch.randn(8, 16).to(device)\n",
        "    assert a.policy(r).shape == (8, 7), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "    assert a.policy_target_net(r).shape == (8, 7), \\\n",
        "    f\"Networks not initialized correctly\"\n",
        "    print(\"Test passed: DDPG nets appear correct!\")\n",
        "DDPG_nets()\n",
        "\n",
        "def test_DDPG_optimizer():\n",
        "    a = DDPG(16, 7)\n",
        "    assert a.q_optimizer is not None, \"Q Optimizer is not initialized\"\n",
        "    assert a.policy_optimizer is not None, \"Policy Optimizer is not initialized\"\n",
        "\n",
        "    assert set(p for p in a.q_net.parameters())  == \\\n",
        "    set(p for group in a.q_optimizer.param_groups for p in group['params']),\\\n",
        "    \"Q optimizer does not match q_net parameters\"\n",
        "\n",
        "    assert set(p for p in a.policy.parameters())  == \\\n",
        "    set(p for group in a.policy_optimizer.param_groups for p in group['params']),\\\n",
        "    \"Policy optimizer does not match policy parameters\"\n",
        "\n",
        "    print(\"Test passed: DDPG optimizer appears correct!\")\n",
        "test_DDPG_optimizer()\n",
        "\n",
        "def DDPG_q_loss():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = (torch.rand((batch_size, n_actions)) - 0.5) * 6\n",
        "    r = torch.rand((batch_size,))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "    not_dones = torch.randint(0, 2, (batch_size,))\n",
        "\n",
        "    ddpg = DDPG(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    ddpg.q_net = nn.Linear(5, 1) # you should not use this architecture..\n",
        "    ddpg.q_target_net = nn.Linear(5, 1)\n",
        "    ddpg.policy = nn.Linear(4, 1)\n",
        "    ddpg.policy_target_net = nn.Linear(4, 1)\n",
        "    loss = ddpg.get_q_loss(s, a, r, s_, not_dones)\n",
        "    # print(loss)\n",
        "    assert abs(loss.item() - (0.6036)) < 1e-4, \\\n",
        "    \"DDPG q loss does not match expected value.\"\n",
        "    print(\"Test passed: DDPG q loss appears correct!\")\n",
        "DDPG_q_loss()\n",
        "\n",
        "def DDPG_policy_loss():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "\n",
        "    ddpg = DDPG(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    ddpg.q_net = nn.Linear(5, 1) # you should not use this architecture..\n",
        "    ddpg.q_target_net = nn.Linear(5, 1)\n",
        "    ddpg.policy = nn.Linear(4, 1)\n",
        "    ddpg.policy_target_net = nn.Linear(4, 1)\n",
        "    loss = ddpg.get_policy_loss(s)\n",
        "    # print(loss)\n",
        "    assert abs(loss.item() - (-0.0553)) < 1e-4, \\\n",
        "    \"DDPG policy loss does not match expected value.\"\n",
        "    print(\"Test passed: DDPG policy loss appears correct!\")\n",
        "DDPG_policy_loss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqCM_Es0jGPs"
      },
      "outputs": [],
      "source": [
        "# DDPG training loop\n",
        "\n",
        "# tensorboard label can be changed with e.g. f'runs/unique_hyperparam_test'\n",
        "writer = SummaryWriter(log_dir=f'runs/DDPG')\n",
        "\n",
        "drl = DRL()\n",
        "ddpg = DDPG(n_obs=4, n_actions=1)\n",
        "\n",
        "# takes ~5-10 minutes on colab gpus\n",
        "for i in range(512):\n",
        "\n",
        "    drl.rollout(ddpg, i)\n",
        "    ddpg.update(drl.replay_buffer, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06u6AzyUxd2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jblevins32/anaconda3/lib/python3.11/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/jblevins32/DRL2 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[122], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m visualize(ddpg)\n",
            "Cell \u001b[0;32mIn[121], line 30\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     29\u001b[0m     actions \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(torch\u001b[38;5;241m.\u001b[39mTensor(obs)\u001b[38;5;241m.\u001b[39mto(device)[\u001b[38;5;28;01mNone\u001b[39;00m, :])[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m obs, _, done, _\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "visualize(ddpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0m3xIFxPsTi"
      },
      "source": [
        "**DDPG Conceptual Question 1 - Optimizers:**\n",
        "\n",
        "In HW1 we used a single combined optimizer for both value and policy nets. For DDPG, we need separate optimizers for Q and Policy nets. Why is that? (hint: policy loss)\n",
        "\n",
        "**Type answer here...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYnwAlKPQKW5"
      },
      "source": [
        "**DDDP Conceptual Question 1 - Replay Buffers:**\n",
        "\n",
        "Every policy rollout (defined in boiler plate code) uses 32 parallel environments simulated for 128 timesteps.\n",
        "```\n",
        "class DRL:\n",
        "    def __init__(self):\n",
        "        self.n_envs = 32\n",
        "        self.n_steps = 128\n",
        "```\n",
        "Additionally, our replay buffer (defined in boiler plate code) is large enough to hold 1,000,000 transitions.\n",
        "```\n",
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        self.buffer = deque(maxlen=1_000_000)\n",
        "        self.batch_size = 32\n",
        "```\n",
        "\n",
        "In 1 or 2 sentences, what might happen to our training speed and stability if we collected less data per rollout and used a smaller replay buffer? Why? Lets say 1 environment, 32 steps, size 32 replay buffer. (Optionally, make these changes in code and observe training results yourselves. Note, if you test lower than 32 transitions you need to reduce ReplayBuffer.batch_size aswell. Comment or revert changes before submitting.)\n",
        "\n",
        "**Type answer here...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuUizaqF_88H"
      },
      "source": [
        "_________________________________\n",
        "# 3. Optional Extra Credit: Soft Actor-Critic (SAC) [10 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7T1jUo_5Vv7"
      },
      "source": [
        "\n",
        "1.   Implement a stochastic policy [4 pts]\n",
        "2.   Implement double Q Learning [3 pts]\n",
        "3.   Implement entropy regularization [3 pts]\n",
        "\n",
        "_________________________________\n",
        "#### Background\n",
        "\n",
        "SAC is a reinforcement learning algorithm that improves DDPG with better stability and exploration. It was introduced in a seminal [publication](https://arxiv.org/pdf/1801.01290) in 2017, and is often considered the go-to model-free off-policy method. Its key innovations relative to DDPG are a stochastic policy, double Q learning, and entropy regularization.\n",
        "\n",
        "We will use SAC to solve a continuous action space environment more robustly than DDPG.\n",
        "________________________________\n",
        "\n",
        "#### DDPG vs SAC\n",
        "\n",
        "**Determinism vs Stochasticity**: DDPG trains a deterministic policy network, which maps states to single continuous actions. Exploration noise has to be injected externally. SAC, on the other hand, learns a stochastic policy represented by a probability distribution over actions. It inherently explores. Additionally, DDPG can overfit to quirks of the q function, which can cause instability, premature convergence, or collapse. In contrast, SAC has stochasticity built into the loss equations, which results in an averaging effect that makes SAC networks less brittle and more stable.\n",
        "\n",
        "**Double Q**: DDPG uses one Q network. Building on the insight from Double DQN ($\\mathbb{E}[\\max(C_1, C_2)] \\geq \\max(\\mathbb{E}[C_1], \\mathbb{E}[C_2])$), SAC learns two separate Q networks, and uses them to mitigate over estimation bias.\n",
        "\n",
        "**Exploration and Entropy**: However exploration noise is injected in DDPG, it is state independent. SAC has adaptive state dependent exploration. As a function of state, its policy outputs the mean and log standard deviation of a guassian policy. The backprop process naturally produces broad guassians when q values are uncertain, and narrow guassians as q values converge. Furthermore, SAC uses entropy regularization to further encourage broad guassians which discourages premature suboptimal convergence.\n",
        "\n",
        "________________________________\n",
        "\n",
        "**Milestone 1 - Stochastic Policy**\n",
        "\n",
        "Re-implement DDPG with a stochastic policy, single q, no entropy.\n",
        "\n",
        "1. Copy-Paste your DDPG code (networks, optimizers, loss functions)\n",
        "  * update policy net output to be twice as large as before\n",
        "\n",
        "2. Finish implementing get_action() and get_target_action()\n",
        " * construct a torch.Normal distribution  using state dependent mean and std_dev\n",
        " * *rsample* actions (has to be rsample not sample for differentiability)\n",
        " * clamp actions to the valid range $[-3, 3]$\n",
        "3. Update DDPG losses for the stochastic policy. These are the same equations as DDPG, but the actions are now stochastically sampled.\n",
        "\n",
        "  * Q Loss\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[Q_\\theta(s, a) - (r_t + \\gamma (Q_{\\theta_\\text{target}}(s', a'))]^2\n",
        "$$\n",
        "where $a'$ comes from calling get_target_action() on $s'$ with noisy=True.\n",
        "\n",
        "  * Policy Loss\n",
        "$$\n",
        "\\mathcal{L}(\\theta_p) = -\\mathbb{E}[Q_\\theta(s, a)]\n",
        "$$\n",
        "where $a$ comes from calling get_action() on $s$ with noisy=True.\n",
        "\n",
        "If you can pass the M1 unit test below, and can run a succesful training at this point, you've earned 4 points!\n",
        "\n",
        "**Milestone 2 - Double Q**\n",
        "\n",
        "Upgrade to Double Q learning for reduced overestimation bias.\n",
        "1. Update Q Networks\n",
        "  * Replace Q and Q_target with Q1, Q2, Q1_target, Q2_target.\n",
        "  * Update q_optimizer to hold parameters for Q1 and Q2\n",
        "2. Update Loss functions\n",
        "  * Q loss : Evaluate both q target nets on $s'$, use the minimum in constructing $q_{\\text{target}}$. Regress both networks to $q_{\\text{target}}$, by adding their MSE losses.\n",
        "$$\n",
        "q_{\\text{target}} = r_t + \\gamma \\min_{i = 1, 2} Q_{\\theta_{\\text{target}, i}}(s', a') \\cdot \\text{not\\_done}\\\\\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{Q_{\\theta_1}(s, a) - q_{\\text{target}}\\}^2] + \\mathbb{E}[\\{Q_{\\theta_2}(s, a) - q_{\\text{target}}\\}^2]\\\\\n",
        "$$\n",
        "  * Policy loss : Same as before but use Q1\n",
        "$$\n",
        "\\mathcal{L}(\\theta_p) = -\\mathbb{E}[Q_{\\theta_1}(s, a)]\n",
        "$$\n",
        "\n",
        "3. Modify the soft target updates in the update function to work for both Q1 and Q2\n",
        "\n",
        "If you can pass the M2 unit test below, and can run a succesful training at this point, you've earned 3 more points! (7 total)\n",
        "\n",
        "**Milestone 3 - Entropy Regularization**\n",
        "\n",
        "Upgrade to Entropy Regularization for better exploration.\n",
        "1. Update Q Loss : add an entropy term to $q_{\\text{target}}$\n",
        "$$\n",
        "q_{\\text{target}} = r_t + \\gamma \\{\\min_{i = 1, 2} Q_{\\theta_{\\text{target}, i}}(s', a')  + \\alpha H(\\pi(s')) \\}\\cdot \\text{not\\_done}\n",
        "$$\n",
        "where $\\alpha$ is a scaling *temperature* and $H$ is entropy of policy $\\pi$ at state $s'$ (hint: get_entropy function)\n",
        "\n",
        "\n",
        "2. Update Policy Loss :\n",
        "$$\n",
        "\\mathcal{L}(\\theta_p) = -\\mathbb{E}[Q_{\\theta_1}(s, a) + \\alpha H(\\pi(s))]\n",
        "$$\n",
        "If you can pass the M3 unit test below, and can run a succesful training at this point, you've earned 3 more points! (10 total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# THIS IS FOR M1\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class SAC:\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        torch.manual_seed(0)\n",
        "        self.alpha = .002\n",
        "\n",
        "        # todo: student code here\n",
        "        self.q_net = nn.Sequential(\n",
        "            nn.Linear(n_obs+n_actions,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        ).to(device)\n",
        "\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(n_obs,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,n_actions*2)\n",
        "        ).to(device)\n",
        "\n",
        "        self.q_target_net = copy.deepcopy(self.q_net).to(device)\n",
        "        self.policy_target_net = copy.deepcopy(self.policy).to(device)\n",
        "\n",
        "        self.q_optimizer = Adam(params=self.q_net.parameters(),lr=3e-3)\n",
        "        self.policy_optimizer = Adam(params=self.policy.parameters(),lr=3e-3)\n",
        "        # end student code\n",
        "\n",
        "    def get_entropy(self, states):\n",
        "        mean, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "        H = Normal(mean, std_dev).entropy()\n",
        "        return H\n",
        "\n",
        "    def get_action(self, states, noisy=False):\n",
        "        mean, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        if noisy == False:\n",
        "            return mean\n",
        "        else:\n",
        "            std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "\n",
        "            #todo: student code\n",
        "            # Get the action\n",
        "            dist = Normal(mean, std_dev)\n",
        "            action = dist.rsample().clamp(-3,3)\n",
        "            return action \n",
        "\n",
        "    def get_target_action(self, states, noisy=False):\n",
        "        mean, log_std_dev = self.policy_target_net(states).chunk(2, dim=-1)\n",
        "        if noisy == False:\n",
        "            return mean\n",
        "        else:\n",
        "            std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "            \n",
        "            #todo: student code\n",
        "            # Get the action\n",
        "            dist = Normal(mean, std_dev)\n",
        "            action = dist.rsample().clamp(-3,3)\n",
        "            return action \n",
        "\n",
        "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, gamma=.99):\n",
        "\n",
        "        #todo: student code here\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 1) Get next actions from target policy\n",
        "            next_actions = self.get_target_action(next_states, noisy=True)\n",
        "\n",
        "            # 2) Get target value from next states and next actions\n",
        "            next_state_action_vec = torch.cat((next_states,next_actions),dim=-1)\n",
        "            q_next = self.q_target_net(next_state_action_vec)\n",
        "\n",
        "        # 3) Get value from current states and current actions\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "        q = self.q_net(state_action_vec)\n",
        "\n",
        "        # 4) Get target q \n",
        "        q_target = rewards.unsqueeze(-1) + gamma*q_next*not_dones.unsqueeze(-1)\n",
        "\n",
        "        # 5) Get q loss\n",
        "        criterion = nn.MSELoss()\n",
        "        Q_loss = criterion(q,q_target)\n",
        "        # end student code\n",
        "\n",
        "        return Q_loss\n",
        "\n",
        "    def get_policy_loss(self, states):\n",
        "        # todo: student code here\n",
        "        # 1) Get actions\n",
        "        actions = self.get_action(states, noisy=True)\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "\n",
        "        policy_loss = -torch.mean(self.q_net(state_action_vec))\n",
        "\n",
        "        # end student code\n",
        "        return policy_loss\n",
        "\n",
        "    def update(self, replay_buffer, i):\n",
        "\n",
        "        for _ in range(64):\n",
        "            loss = self.get_q_loss(*replay_buffer.sample())\n",
        "            self.q_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.q_optimizer.step()\n",
        "        writer.add_scalar(\"loss/q loss\", loss.item(), i)\n",
        "\n",
        "        for _ in range(4):\n",
        "            states, _, _, _, _ = replay_buffer.sample()\n",
        "            loss = self.get_policy_loss(states)\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.policy_optimizer.step()\n",
        "        writer.add_scalar(\"loss/ - policy loss\", -loss.item(), i)\n",
        "\n",
        "        # exploration rate logging\n",
        "        with torch.no_grad():\n",
        "            _, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "        writer.add_scalar(\"stats/exploration rate\", std_dev.mean().item(), i)\n",
        "\n",
        "        tau = 0.1  # Soft update factor  # student code here for M2\n",
        "        for target_param, param in zip(self.q_target_net.parameters(), self.q_net.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(self.policy_target_net.parameters(), self.policy.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GshOb-g03-Xd"
      },
      "outputs": [],
      "source": [
        "# THIS IS FOR M2\n",
        "\n",
        "from torch.distributions import Normal\n",
        "\n",
        "class SAC:\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        torch.manual_seed(0)\n",
        "        self.alpha = .002\n",
        "\n",
        "        # todo: student code here\n",
        "        self.q_1_net = nn.Sequential(\n",
        "            nn.Linear(n_obs+n_actions,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,1)\n",
        "        ).to(device)\n",
        "\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(n_obs,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,n_actions*2)\n",
        "        ).to(device)\n",
        "\n",
        "        self.q_2_net = copy.deepcopy(self.q_1_net).to(device)\n",
        "        self.q_1_target_net = copy.deepcopy(self.q_1_net).to(device)\n",
        "        self.q_2_target_net = copy.deepcopy(self.q_1_net).to(device)\n",
        "        self.policy_target_net = copy.deepcopy(self.policy).to(device)\n",
        "\n",
        "        params_combined = list(self.q_1_net.parameters()) + list(self.q_2_net.parameters())\n",
        "        self.q_optimizer = Adam(params=params_combined,lr=3e-3)\n",
        "        self.policy_optimizer = Adam(params=self.policy.parameters(),lr=3e-3)\n",
        "        # end student code\n",
        "\n",
        "    def get_entropy(self, states):\n",
        "        mean, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "        H = Normal(mean, std_dev).entropy()\n",
        "        return H\n",
        "\n",
        "    def get_action(self, states, noisy=False):\n",
        "        mean, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        if noisy == False:\n",
        "            return mean\n",
        "        else:\n",
        "            std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "\n",
        "            #todo: student code\n",
        "            # Get the action\n",
        "            dist = Normal(mean, std_dev)\n",
        "            action = dist.rsample().clamp(-3,3)\n",
        "            return action \n",
        "\n",
        "    def get_target_action(self, states, noisy=False):\n",
        "        mean, log_std_dev = self.policy_target_net(states).chunk(2, dim=-1)\n",
        "        if noisy == False:\n",
        "            return mean\n",
        "        else:\n",
        "            std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "            \n",
        "            #todo: student code\n",
        "            # Get the action\n",
        "            dist = Normal(mean, std_dev)\n",
        "            action = dist.rsample().clamp(-3,3)\n",
        "            return action \n",
        "\n",
        "    def get_q_loss(self, states, actions, rewards, next_states, not_dones, gamma=.99):\n",
        "\n",
        "        #todo: student code here\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # 1) Get next actions from target policy\n",
        "            next_actions = self.get_target_action(next_states, noisy=True)\n",
        "\n",
        "            # 2) Get target value from next states and next actions\n",
        "            next_state_action_vec = torch.cat((next_states,next_actions),dim=-1)\n",
        "            q_1_next = self.q_1_target_net(next_state_action_vec)\n",
        "            q_2_next = self.q_2_target_net(next_state_action_vec)\n",
        "\n",
        "        # 3) Get value from current states and current actions\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "        q_1 = self.q_1_net(state_action_vec)\n",
        "        q_2 = self.q_2_net(state_action_vec)\n",
        "\n",
        "        # 4) Get target q \n",
        "        q_next = torch.min(q_1_next,q_2_next)\n",
        "        q_target = rewards.unsqueeze(-1) + gamma*q_next*not_dones.unsqueeze(-1)\n",
        "\n",
        "        # 5) Get q loss\n",
        "        criterion = nn.MSELoss()\n",
        "        Q_loss = criterion(q_1,q_target) + criterion(q_2,q_target)\n",
        "        # end student code\n",
        "\n",
        "        return Q_loss\n",
        "\n",
        "    def get_policy_loss(self, states):\n",
        "        # todo: student code here\n",
        "        # 1) Get actions\n",
        "        actions = self.get_action(states, noisy=True)\n",
        "        state_action_vec = torch.cat((states,actions),dim=-1)\n",
        "\n",
        "        policy_loss = -torch.mean(self.q_1_net(state_action_vec))\n",
        "\n",
        "        # end student code\n",
        "        return policy_loss\n",
        "\n",
        "    def update(self, replay_buffer, i):\n",
        "\n",
        "        for _ in range(64):\n",
        "            loss = self.get_q_loss(*replay_buffer.sample())\n",
        "            self.q_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.q_optimizer.step()\n",
        "        writer.add_scalar(\"loss/q loss\", loss.item(), i)\n",
        "\n",
        "        for _ in range(4):\n",
        "            states, _, _, _, _ = replay_buffer.sample()\n",
        "            loss = self.get_policy_loss(states)\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.policy_optimizer.step()\n",
        "        writer.add_scalar(\"loss/ - policy loss\", -loss.item(), i)\n",
        "\n",
        "        # exploration rate logging\n",
        "        with torch.no_grad():\n",
        "            _, log_std_dev = self.policy(states).chunk(2, dim=-1)\n",
        "        std_dev = log_std_dev.exp().clamp(.2, 2)\n",
        "        writer.add_scalar(\"stats/exploration rate\", std_dev.mean().item(), i)\n",
        "\n",
        "        tau = 0.1  # Soft update factor  # student code here for M2\n",
        "        for target_param, param in zip(self.q_1_target_net.parameters(), self.q_1_net.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(self.q_2_target_net.parameters(), self.q_2_net.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "            \n",
        "        for target_param, param in zip(self.policy_target_net.parameters(), self.policy.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14T4mDGEgKak"
      },
      "source": [
        "Here's some SAC unit tests. Its not possible to pass them all with the same code. You can either make separate classes to pass each one, or simply edit the one SAC class repeatedly to get the highest milestone. You get cumulative credit for the highest milestone you acheive. Work on M1, then once you pass, work on M2, then M3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "Xk9djIsdVWV2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: SAC M1 q loss appears correct!\n",
            "Test passed: SAC M1 policy loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title SAC Milestone 1 loss unit tests\n",
        "def SAC_M1_losses():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = (torch.rand((batch_size, n_actions)) - 0.5) * 6\n",
        "    r = torch.rand((batch_size,))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "    not_dones = torch.randint(0, 2, (batch_size,))\n",
        "\n",
        "    sac = SAC(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    sac.q_net = nn.Linear(5, 1) # you should not use this architecture..\n",
        "    sac.q_target_net = nn.Linear(5, 1)\n",
        "    sac.policy = nn.Linear(4, 2)\n",
        "    sac.policy_target_net = nn.Linear(4, 2)\n",
        "    q_loss = sac.get_q_loss(s, a, r, s_, not_dones)\n",
        "    # print(q_loss)\n",
        "    assert abs(q_loss.item() - (0.7857)) < 1e-4, \\\n",
        "    \"SAC M1 q loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M1 q loss appears correct!\")\n",
        "\n",
        "    p_loss = sac.get_policy_loss(s)\n",
        "    # print(p_loss)\n",
        "    assert abs(p_loss.item() - (0.2232)) < 1e-4, \\\n",
        "    \"SAC M1 policy loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M1 policy loss appears correct!\")\n",
        "\n",
        "SAC_M1_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "cellView": "form",
        "id": "Q1ZxVDKggk3f"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(p_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.1319\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC M2 policy loss does not match expected value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: SAC M2 policy loss appears correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m SAC_M2_losses()\n",
            "Cell \u001b[0;32mIn[32], line 22\u001b[0m, in \u001b[0;36mSAC_M2_losses\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m sac\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m sac\u001b[38;5;241m.\u001b[39mpolicy_target_net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m q_loss \u001b[38;5;241m=\u001b[39m sac\u001b[38;5;241m.\u001b[39mget_q_loss(s, a, r, s_, not_dones)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(q_loss)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(q_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1.0490\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC M2 q loss does not match expected value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "Cell \u001b[0;32mIn[28], line 92\u001b[0m, in \u001b[0;36mSAC.get_q_loss\u001b[0;34m(self, states, actions, rewards, next_states, not_dones, gamma)\u001b[0m\n\u001b[1;32m     89\u001b[0m Q_loss \u001b[38;5;241m=\u001b[39m criterion(q_1,q_target) \u001b[38;5;241m+\u001b[39m criterion(q_2,q_target)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# end student code\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q_loss\n",
            "Cell \u001b[0;32mIn[28], line 92\u001b[0m, in \u001b[0;36mSAC.get_q_loss\u001b[0;34m(self, states, actions, rewards, next_states, not_dones, gamma)\u001b[0m\n\u001b[1;32m     89\u001b[0m Q_loss \u001b[38;5;241m=\u001b[39m criterion(q_1,q_target) \u001b[38;5;241m+\u001b[39m criterion(q_2,q_target)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# end student code\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q_loss\n",
            "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1368\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:1311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_wait_suspend(thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mwait(wait_timeout)\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title SAC Milestone 2 loss unit tests\n",
        "def SAC_M2_losses():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = (torch.rand((batch_size, n_actions)) - 0.5) * 6\n",
        "    r = torch.rand((batch_size,))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "    not_dones = torch.randint(0, 2, (batch_size,))\n",
        "\n",
        "    sac = SAC(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    sac.q1_net = nn.Linear(5, 1) # you should not use this architecture..\n",
        "    sac.q1_target_net = nn.Linear(5, 1)\n",
        "    sac.q2_net = nn.Linear(5, 1)\n",
        "    sac.q2_target_net = nn.Linear(5, 1)\n",
        "    sac.policy = nn.Linear(4, 2)\n",
        "    sac.policy_target_net = nn.Linear(4, 2)\n",
        "    q_loss = sac.get_q_loss(s, a, r, s_, not_dones)\n",
        "    # print(q_loss)\n",
        "    assert abs(q_loss.item() - (1.0490)) < 1e-4, \\\n",
        "    \"SAC M2 q loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M2 q loss appears correct!\")\n",
        "\n",
        "    p_loss = sac.get_policy_loss(s)\n",
        "    # print(p_loss)\n",
        "    assert abs(p_loss.item() - (-0.1319)) < 1e-4, \\\n",
        "    \"SAC M2 policy loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M2 policy loss appears correct!\")\n",
        "\n",
        "SAC_M2_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XaNVfRDQiCUE"
      },
      "outputs": [],
      "source": [
        "# @title SAC Milestone 3 loss unit tests\n",
        "def SAC_M3_losses():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 5, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = (torch.rand((batch_size, n_actions)) - 0.5) * 6\n",
        "    r = torch.rand((batch_size,))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "    not_dones = torch.randint(0, 2, (batch_size,))\n",
        "\n",
        "    sac = SAC(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    sac.q1_net = nn.Linear(5, 1) # you should not use this architecture..\n",
        "    sac.q1_target_net = nn.Linear(5, 1)\n",
        "    sac.q2_net = nn.Linear(5, 1)\n",
        "    sac.q2_target_net = nn.Linear(5, 1)\n",
        "    sac.policy = nn.Linear(4, 2)\n",
        "    sac.policy_target_net = nn.Linear(4, 2)\n",
        "    q_loss = sac.get_q_loss(s, a, r, s_, not_dones)\n",
        "    # print(q_loss)\n",
        "    assert abs(q_loss.item() - (1.0530)) < 1e-4, \\\n",
        "    \"SAC M3 q loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M3 q loss appears correct!\")\n",
        "\n",
        "    p_loss = sac.get_policy_loss(s)\n",
        "    # print(p_loss)\n",
        "    assert abs(p_loss.item() - (-0.1341)) < 1e-4, \\\n",
        "    \"SAC M3 policy loss does not match expected value.\"\n",
        "    print(\"Test passed: SAC M3 policy loss appears correct!\")\n",
        "\n",
        "SAC_M3_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xAfqllvEF9v"
      },
      "outputs": [],
      "source": [
        "# run this for whatever highest milestone you reach\n",
        "writer = SummaryWriter(log_dir=f'runs/SAC')\n",
        "\n",
        "drl = DRL()\n",
        "sac = SAC(n_obs=4, n_actions=1)\n",
        "\n",
        "# takes ~5-10 minutes on colab gpus\n",
        "for i in range(512):\n",
        "\n",
        "    drl.rollout(sac, i)\n",
        "    sac.update(drl.replay_buffer, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQFm0-PQG6U4"
      },
      "outputs": [],
      "source": [
        "visualize(sac)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Cqv2aln3DDCz",
        "hdzpOoeBR9Ql",
        "RWpQBY3YKGAX",
        "2ZXqBcnlJbar",
        "kEHCeEYVJsB8",
        "PWsqAz6VRypQ",
        "qItDHaIL_z1Y",
        "pQ1i0MUp_3rT",
        "PuUizaqF_88H"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
